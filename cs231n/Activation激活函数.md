# Activation激活函数

使用激活函数一般使用以0为中心的函数，但是会导致的**现象**是：

1. 被激活的值趋向于0，当backward时，激活函数的输出作为下一层的输入X，所以当计算下一层的W的梯度时$$\frac{dL}{dW_n} = \frac{dL}{dZ} * X$$，所以当X趋向于0时，W的梯度也趋向于0，更新速度缓慢。
2. 前一层的输出是$$W_i * X + b = Z$$，同时也是下一层的输入$$W_{i+1} * Z + b = Q$$，当梯度传播到i+1层时值为F，则第i层Z的梯度则为 $$F * \frac{dQ}{dZ} = F * W_{i+1}$$ ， 所以传递的参数值一直是从后向前的W的乘积，如果过深，神容易造成传递到前面层的梯度值为0或过大。（**做上层梯度和权重W的乘法得到下一层的梯度**）

如果使用大的W代替较小的W，会使得W*X + b = Z，得到较大的Z或较小的Z，则tanh(Z)则易饱和，梯度为0. 

So, 为什么激活后的值以0为中心有什么好处。



# Batch Normalization

首先计算所有数据集在每个维度的均值与方差。

<img src="C:\Users\DianwenMei\AppData\Roaming\Typora\typora-user-images\image-20201008212444447.png" alt="image-20201008212444447" style="zoom:80%;" />

一般可以消除梯度消失或爆炸的问题，，

