# RNN

## RNN的动机

最早使用在NLP。当预测一段话中的某个单词时，传统方法是使用N-Gram，即当前词与前面N个单词有关，靠这N个单词预测出想要得到的单词。但是如果想提高精度就需要增大N，由于句子不是定长的，所以N没办法确定，且N越大计算量越大。于是RNN出现了。RNN理论上可以往前看任意个词。

## RNN是什么

<img src="https://pic2.zhimg.com/80/v2-b0175ebd3419f9a11a3d0d8b00e28675_720w.jpg" alt="img" style="zoom:50%;" />

<img src="https://pic4.zhimg.com/v2-9524a28210c98ed130644eb3c3002087_r.jpg" alt="preview" style="zoom:50%;" />

一个简单的RNN网络由一个输入层、隐藏层、输出层。每一个隐藏层都融合了当前单词与前一个隐藏层，所以每个$$S_t$$隐藏层包含了当前单词与之前所有单词的信息，然后用当前的隐藏层$$S_t$$预测下一个单词。所以预测此单此时使用了之前的所有单词信息。

所以**隐藏层的作用的是 **：将当前的信息与之前的所有信息融合



**问题**：S的t时刻与t-1时刻直接相关，但是与之前时刻的相关性会逐渐减弱，所以存在==遗忘问题==，因为之前的单词可能会有很大的一个权重。

